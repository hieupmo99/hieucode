# Quick Reference: File Relationships

## ğŸ¯ One-Line Summaries

| File | What It Does |
|------|--------------|
| **mass_crawling.py** | Selenium crawler that scrapes VnExpress and sends to Kafka |
| **producer.py** | Kafka producer helper (creates connection, sends messages) |
| **spark_app.py** | Reads from Kafka, processes, saves to SQLite database |
| **daily_crawler.py** | Airflow DAG that auto-starts services and runs crawler daily |
| **server.py** | Flask API backend for dashboard (stats, controls, logs) |
| **dashboard_enhanced.html** | Web UI with 4 tabs (Overview, Services, Logs, Articles) |
| **docker-compose.yml** | Orchestrates all 11 containers (Kafka, Spark, Airflow, etc.) |
| **hieudb.db** | SQLite database storing 7,534+ crawled articles |
| **Dockerfiles** | Build images for crawler, Spark, and Airflow containers |

---

## ğŸ”„ Data Flow (Simplified)

```
VnExpress.net
     â†“
mass_crawling.py (Selenium scraper)
     â†“
producer.py (Kafka producer)
     â†“
Kafka Cluster (3 brokers, vnexpress_topic)
     â†“
spark_app.py (Spark Streaming)
     â†“
hieudb.db (SQLite database)
     â†“
server.py (Flask API)
     â†“
dashboard_enhanced.html (Web UI)
     â†“
ğŸ‘¤ You see articles in browser
```

---

## ğŸ­ Who Talks to Who?

### **mass_crawling.py** talks to:
- âœ… VnExpress website (scrapes)
- âœ… producer.py (imports)
- âœ… Kafka cluster (sends messages)

### **spark_app.py** talks to:
- âœ… Kafka cluster (reads messages)
- âœ… hieudb.db (writes articles)

### **daily_crawler.py** (Airflow DAG) talks to:
- âœ… Docker (starts containers)
- âœ… Kafka (health checks)
- âœ… mass_crawling.py (executes)

### **server.py** talks to:
- âœ… hieudb.db (queries)
- âœ… Docker (starts/stops services)
- âœ… dashboard_enhanced.html (serves page)

### **dashboard_enhanced.html** talks to:
- âœ… server.py (API calls)
- âœ… Your browser (displays UI)

### **docker-compose.yml** talks to:
- âœ… All Dockerfiles (builds images)
- âœ… All services (orchestrates)

---

## ğŸ—ï¸ Component Ownership

```
Crawler Layer:
â”œâ”€â”€ mass_crawling.py        (main crawler script)
â”œâ”€â”€ producer.py             (Kafka producer helper)
â”œâ”€â”€ crawling.py             (standalone test version)
â””â”€â”€ app/Dockerfile          (builds crawler container)

Streaming Layer:
â”œâ”€â”€ spark_app.py            (Spark streaming processor)
â”œâ”€â”€ spark/Dockerfile        (builds Spark container)
â””â”€â”€ sqlite-jdbc.jar         (JDBC driver for SQLite)

Storage Layer:
â””â”€â”€ hieudb.db              (SQLite database)

Orchestration Layer:
â”œâ”€â”€ daily_crawler.py        (Airflow DAG)
â”œâ”€â”€ airflow/Dockerfile      (builds Airflow container)
â””â”€â”€ docker-compose.yml      (all services)

Monitoring Layer:
â”œâ”€â”€ server.py               (Flask API)
â””â”€â”€ dashboard_enhanced.html (Web UI)

Documentation:
â”œâ”€â”€ ARCHITECTURE.md         (detailed guide)
â”œâ”€â”€ DASHBOARD_USAGE.md      (dashboard guide)
â”œâ”€â”€ AIRFLOW_AUTO_START.md   (DAG guide)
â””â”€â”€ QUICK_REFERENCE.md      (this file)
```

---

## ğŸš¦ Execution Paths

### **Path 1: Automatic Daily Crawl**
```
06:00 AM
  â†“
Airflow Scheduler wakes up
  â†“
daily_crawler.py triggers
  â†“
Starts: Kafka, Spark, Crawler (parallel)
  â†“
Checks: Kafka health
  â†“
Executes: mass_crawling.py
  â†“
Crawls VnExpress â†’ Kafka â†’ Spark â†’ Database
  â†“
Done! âœ…
```

### **Path 2: Manual Dashboard Start**
```
You click "Start Crawler" in Dashboard
  â†“
dashboard_enhanced.html â†’ POST /api/crawler/start
  â†“
server.py â†’ docker exec crawler python mass_crawling.py
  â†“
Crawls VnExpress â†’ Kafka â†’ Spark â†’ Database
  â†“
Dashboard refreshes stats every 5 seconds
  â†“
You see article count increasing âœ…
```

### **Path 3: Manual Command**
```
You run: docker exec crawler python /opt/app/mass_crawling.py
  â†“
Crawls VnExpress â†’ Kafka â†’ Spark â†’ Database
  â†“
Check dashboard to see new articles âœ…
```

---

## ğŸ”Œ Ports Reference

| Service | Port | Purpose |
|---------|------|---------|
| Kafka-1 | 19092 | Kafka broker 1 (external) |
| Kafka-2 | 29092 | Kafka broker 2 (external) |
| Kafka-3 | 39092 | Kafka broker 3 (external) |
| Kafka UI | 8080 | Web UI for Kafka monitoring |
| Airflow | 8081 | Airflow web UI (admin/admin) |
| Dashboard | 5000 | Flask dashboard UI |

---

## ğŸ“¦ Container Network

All containers are on `kafka-net` network:

```
kafka-net (bridge network)
â”œâ”€â”€ kafka-1          (hostname: kafka-1)
â”œâ”€â”€ kafka-2          (hostname: kafka-2)
â”œâ”€â”€ kafka-3          (hostname: kafka-3)
â”œâ”€â”€ crawler          (hostname: crawler)
â”œâ”€â”€ spark            (hostname: spark)
â”œâ”€â”€ airflow-webserver
â”œâ”€â”€ airflow-scheduler
â”œâ”€â”€ postgres-airflow
â””â”€â”€ kafka-ui
```

Services communicate using container names:
- `kafka-1:9092` instead of `localhost:19092`
- `spark` instead of `localhost`

---

## ğŸ—„ï¸ Database Schema

```sql
CREATE TABLE articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT,
    link TEXT UNIQUE,              -- Prevents duplicates
    image TEXT,
    description TEXT,
    category TEXT,                  -- thoi-su, giai-tri, etc.
    timestamp INTEGER               -- Unix timestamp
);
```

**Current Stats**:
- Total: 7,534 articles
- Today: 2,457 articles
- This week: 2,457 articles

---

## ğŸ¯ Key Kafka Details

**Topic**: `vnexpress_topic`
**Partitions**: 3 (distributed across 3 brokers)
**Format**: JSON messages

**Message Structure**:
```json
{
  "title": "Article title",
  "link": "https://vnexpress.net/...",
  "image": "https://...",
  "description": "Article summary",
  "category": "thoi-su",
  "timestamp": 1702809600
}
```

---

## ğŸ”§ Common Commands

### Start Everything:
```bash
cd ~/Documents/hieucode
docker-compose up -d
```

### Stop Everything:
```bash
docker-compose down
```

### Start Crawler:
```bash
# Option 1: Dashboard (http://localhost:5000)
# Option 2: Airflow (http://localhost:8081)
# Option 3: Command
docker exec crawler python /opt/app/mass_crawling.py
```

### Check Logs:
```bash
docker logs crawler --tail 50
docker logs spark --tail 50
docker logs kafka-1 --tail 50
```

### Check Database:
```bash
sqlite3 ~/Documents/hieucode/app/hieudb.db
.tables                         # Show tables
SELECT COUNT(*) FROM articles;  # Total count
SELECT * FROM articles LIMIT 5; # Show recent
.quit
```

### Trigger Airflow DAG:
```bash
docker exec airflow-scheduler airflow dags trigger vnexpress_daily_crawler
```

---

## ğŸ¬ What Happens When...

### **...you run `docker-compose up -d`:**
1. Starts Kafka cluster (3 brokers)
2. Starts PostgreSQL for Airflow
3. Starts Airflow webserver & scheduler
4. Starts Kafka UI
5. Starts Spark (begins streaming immediately)
6. Starts Crawler container (idle, waits for commands)

### **...you start the crawler:**
1. Chrome browser launches (headless)
2. Navigates to VnExpress categories
3. Scrolls down to load more articles
4. Extracts article data
5. Sends each article to Kafka
6. Continues until stopped

### **...Kafka receives a message:**
1. Assigns to one of 3 partitions
2. Replicates across brokers
3. Stores in partition log
4. Waits for consumers

### **...Spark reads from Kafka:**
1. Consumes messages in 10-second batches
2. Parses JSON to DataFrame
3. Checks if article already exists (by link)
4. Inserts new articles to SQLite
5. Skips duplicates

### **...you open the dashboard:**
1. Browser requests `http://localhost:5000`
2. server.py serves dashboard_enhanced.html
3. JavaScript loads and calls `/api/stats`
4. server.py queries hieudb.db
5. Returns JSON with counts
6. Dashboard displays stats
7. Refreshes every 5 seconds

---

## ğŸ“ Learning Path

**If you want to understand**:

**1. How scraping works** â†’ Read `mass_crawling.py`
- See Selenium usage
- Understand scroll and extract logic

**2. How Kafka works** â†’ Check `producer.py` and Kafka UI
- See producer configuration
- View topics and partitions at http://localhost:8080

**3. How Spark processes data** â†’ Read `spark_app.py`
- See Kafka integration
- Understand streaming batches
- Check SQLite write logic

**4. How automation works** â†’ Read `daily_crawler.py`
- See Airflow task definitions
- Understand DAG dependencies
- Check service startup logic

**5. How monitoring works** â†’ Read `server.py` and `dashboard_enhanced.html`
- See Flask API endpoints
- Check database queries
- Understand JavaScript fetch calls

**6. How everything connects** â†’ Read `docker-compose.yml`
- See all service definitions
- Understand networks and volumes
- Check environment variables

---

## ğŸ† Success Indicators

**âœ… Everything is working when**:
- Dashboard shows increasing article count
- `docker ps` shows 11 containers running
- Airflow UI shows green DAG runs
- Kafka UI shows messages in `vnexpress_topic`
- Database grows: `SELECT COUNT(*) FROM articles`
- No errors in logs: `docker logs <container>`

---

**Quick Reference Created**: December 17, 2025
