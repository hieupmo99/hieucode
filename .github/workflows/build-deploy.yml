name: Build and Deploy Pipeline

on:
  push:
    branches: [main]
    paths:
      - 'app/**'
      - 'dags/**'
      - 'airflow/**'
      - 'spark/**'
      - 'docker-compose.yml'
      - '.github/workflows/build-deploy.yml'
  workflow_dispatch:
    inputs:
      run_crawler:
        description: 'Run crawler after build'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

jobs:
  # Job 1: Build Docker Images
  build-images:
    name: Build Docker Images
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Crawler Image
      run: |
        echo "üî® Building crawler image..."
        docker build -t hieucode-crawler:latest -f app/Dockerfile ./app
        echo "‚úÖ Crawler image built"
    
    - name: Build Spark Image
      run: |
        echo "üî® Building Spark image..."
        docker build -t hieucode-spark:latest -f spark/Dockerfile .
        echo "‚úÖ Spark image built"
    
    - name: Build Airflow Image
      run: |
        echo "üî® Building Airflow image..."
        docker build -t hieucode-airflow:latest -f airflow/Dockerfile .
        echo "‚úÖ Airflow image built"
    
    - name: Test Docker Compose Config
      run: |
        echo "‚úÖ Validating docker-compose.yml..."
        docker compose config > /dev/null
        echo "‚úÖ Docker compose configuration is valid"

  # Job 2: Deploy to Test Environment
  deploy-test:
    name: Deploy & Test Pipeline
    runs-on: ubuntu-latest
    needs: build-images
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Create required directories
      run: |
        mkdir -p app logs dags airflow
        chmod 777 logs
    
    - name: Start Kafka Cluster
      run: |
        echo "üöÄ Starting Kafka cluster..."
        docker compose up -d kafka-1 kafka-2 kafka-3
        echo "‚è≥ Waiting for Kafka to be ready..."
        sleep 30
    
    - name: Verify Kafka Health
      run: |
        echo "üîç Checking Kafka health..."
        docker compose ps
        docker compose logs kafka-1 | tail -20
        echo "‚úÖ Kafka cluster is running"
    
    - name: Create Kafka Topics
      run: |
        echo "üìä Creating vnexpress_topic..."
        docker compose exec -T kafka-1 /opt/kafka/bin/kafka-topics.sh \
          --create \
          --topic vnexpress_topic \
          --bootstrap-server localhost:9092 \
          --partitions 3 \
          --replication-factor 2 \
          --if-not-exists || echo "Topic already exists"
        
        echo "‚úÖ Kafka topics ready"
    
    - name: Start Spark Streaming
      run: |
        echo "üöÄ Starting Spark..."
        docker compose up -d spark
        sleep 10
        docker compose logs spark | tail -20
        echo "‚úÖ Spark started"
    
    - name: Run Test Crawler
      if: github.event.inputs.run_crawler != 'false'
      run: |
        echo "üï∑Ô∏è Running test crawler..."
        docker compose run --rm crawler python3 mass_crawling.py || echo "Crawler test completed"
        
        echo "‚è≥ Waiting for Spark to process..."
        sleep 15
        
        echo "‚úÖ Crawler test completed"
    
    - name: Verify Pipeline Results
      run: |
        echo "üîç Checking results..."
        
        # Check if SQLite database was created
        if docker compose exec -T spark ls /opt/app/hieudb.db 2>/dev/null; then
          echo "‚úÖ Database created"
          docker compose exec -T spark sqlite3 /opt/app/hieudb.db "SELECT COUNT(*) as total FROM vnexpress;" || echo "Checking database..."
        else
          echo "‚ö†Ô∏è  Database not found (may need more time)"
        fi
    
    - name: Show Logs
      if: always()
      run: |
        echo "üìã Kafka Logs:"
        docker compose logs kafka-1 --tail=50
        echo ""
        echo "üìã Spark Logs:"
        docker compose logs spark --tail=50
    
    - name: Cleanup
      if: always()
      run: |
        echo "üßπ Cleaning up..."
        docker compose down -v
        echo "‚úÖ Cleanup complete"

  # Job 3: Notify Completion
  notify:
    name: Notify Deployment Status
    runs-on: ubuntu-latest
    needs: [build-images, deploy-test]
    if: always()
    
    steps:
    - name: Check Results
      run: |
        echo "============================================"
        echo "üìä Pipeline Deployment Summary"
        echo "============================================"
        echo ""
        echo "Build Status: ${{ needs.build-images.result }}"
        echo "Deploy Status: ${{ needs.deploy-test.result }}"
        echo ""
        
        if [ "${{ needs.build-images.result }}" == "success" ] && [ "${{ needs.deploy-test.result }}" == "success" ]; then
          echo "‚úÖ All jobs completed successfully!"
          echo ""
          echo "üéØ Next Steps:"
          echo "1. Docker images are built and tested"
          echo "2. Pull latest code: git pull"
          echo "3. Rebuild locally: docker compose build"
          echo "4. Start pipeline: ./pipeline.sh start"
          echo "5. Access Airflow: http://localhost:8081"
        else
          echo "‚ö†Ô∏è  Some jobs failed. Check logs above."
        fi
